# ETL Pipeline for Sparkify to analyze Song Play data  
  
#### About this project  
  
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.  
  
The files contained within this project create a database schema and ETL pipeline to create a Postgres database with tables designed to opimize queries on song play analysis.  
  
#### The Song Play data    
  
The data provided consist of 2 folders containing JSON files with the following data:    
  
1.  Song Dataset    
  
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.  
  
2.  Log Dataset  
  
The second dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above. This is simulated activity logs from a music streaming app based on specified configurations.  
  
#### Schema    
  
The schema created has the following fact and dimension tables in order to optimize queries for song play analysis, allowing analysis of song plays with as few reads and joins as possible:    
  
**Fact Table**  
  
*Songplay*  
  
Songplay is the fact table.  Records in log data associated with song plays i.e. records with page = "NextSong".  The table contains the following columns:  
  
songplay_id: Unique ID for each user, each time they play a song  
start_time: The start time for the song (in milliseconds)  
userId: The unique ID for the user  
level: Indicates if the user is free or paid user  
song_id: Unique ID for a song  
artist_id: Unique ID for an artist  
sessionId: Unique ID for an individual user session  
location: City and State of the user  
userAgent: Browser and oprating system used  
  
**Dimension Tables**  
  
*Users*  
  
Information about users in the app  
  
userId: Unique ID for each user   
firstName: Firstname of the user  
lastName:  Lastname of the user  
gender:  Gender of the user  
level: Indicates if the user is free or paid  
  
*Songs*  
  
Songs in the music database  
  
song_id: Unique ID for each song  
title: Song title  
artist_id: Unique ID for each artist  
year: Year the song was released  
duration: Length of the song  
  
*Artists* 
  
Artists in music database  
  
artist_id: Unique ID for each artist  
artist_name: Name of the artist  
artist_location: Location of the artist  
artist_latitude: Latitude of the artist  
artist_longitude: Longitude of the artist  
  
*Time*  
  
Timestamps of records in songplays broken down into specific units  
  
start_time: Timestamp in milliseconds   
hour: Hour of the timestamp  
day: Day of the timestamp  
week: Week of the timestamp  
month: Month of the timestamp  
year: Year of the timestamp  
weekday: Weekday of the timestamp  
  

#### How to run the code

The following files are included in order to create the database and run the ETL:

*Create_tables.py*  

This code must be run first, it creates the database if it does not exist and then connects to it.  It then drops and creates the empty tables that will hold the data and are populated in the ETL.  

*sql_queries.py*  

Contains the SQL queries to drop, create and insert data into the tables.  

*etl.py*  

The main ETL script that scans the data folder for songs and log files and populates the database.  

*test.ipynb*  

Contains some code to test tables are created and data is inserted as expected.  

*etl.ipynb*  

A helpful notebook to understand how the code in etl.py is working.  